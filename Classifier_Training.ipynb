{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7eb1a12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 12:03:19.695923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 12:03:19.730870: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"TKAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE # prompts the tf.data runtime to tune the value dynamically at runtime\n",
    "\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518d677b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "def create_dataset_from_path(path):\n",
    "\n",
    "    # function that converts a file path to an (img, label) pair\n",
    "    def get_label(file_path):\n",
    "      # Convert the path to a list of path components\n",
    "      parts = tf.strings.split(file_path, os.path.sep)\n",
    "      # The second to last is the class-directory\n",
    "      one_hot = parts[-2] == class_names\n",
    "      # Integer encode the label\n",
    "      return tf.argmax(one_hot)\n",
    "\n",
    "    def decode_img(img):\n",
    "      # Convert the compressed string to a 3D uint8 tensor\n",
    "      img = tf.io.decode_jpeg(img, channels=1)\n",
    "      # Resize the image to the desired size\n",
    "      return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "    def process_path(file_path):\n",
    "      label = get_label(file_path)\n",
    "      # Load the raw data from the file as a string\n",
    "      img = tf.io.read_file(file_path)\n",
    "      img = decode_img(img)\n",
    "      return img, label\n",
    "\n",
    "    # Make the imgs in to batches\n",
    "    def configure_for_performance(ds):\n",
    "      ds = ds.cache()\n",
    "      ds = ds.shuffle(buffer_size=1000)\n",
    "      ds = ds.batch(batch_size)\n",
    "      ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "      return ds\n",
    "\n",
    "    data_dir = pathlib.Path(path) # for pycharm\n",
    "    #data_dir = pathlib.Path('mnist_png/training') # for the jupyter notebook\n",
    "\n",
    "    print(path+'/*/*')\n",
    "\n",
    "    # Shuffle might be better set to False but this seems better for now\n",
    "    list_ds = tf.data.Dataset.list_files((path+'/*/*'), shuffle=True)\n",
    "\n",
    "    image_count = len(list_ds)\n",
    "    class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
    "    print(class_names)\n",
    "    val_size = int(image_count * 0.2)\n",
    "    train_ds = list_ds.skip(val_size) # skips x initial elements from this dataset.\n",
    "    val_ds = list_ds.take(val_size)\n",
    "    print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "    print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "\n",
    "    # Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "    train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    # normalized_ds  -> range goes from 0 to 1\n",
    "    # train_ds  -> range goes from 0 to\n",
    "\n",
    "    val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    normalized_ds = configure_for_performance(normalized_ds)\n",
    "    val_ds = configure_for_performance(val_ds)\n",
    "\n",
    "    print(normalized_ds)\n",
    "    return normalized_ds, val_ds, class_names\n",
    "\n",
    "\n",
    "def create_dataset_from_img(path):\n",
    "\n",
    "    def decode_img(img):\n",
    "      # Convert the compressed string to a 3D uint8 tensor\n",
    "      img = tf.io.decode_jpeg(img, channels=1)\n",
    "      # Resize the image to the desired size\n",
    "      return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "    def process_path(file_path):\n",
    "      label = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "      #print(label)\n",
    "      # Load the raw data from the file as a string\n",
    "      img = tf.io.read_file(file_path)\n",
    "      img = decode_img(img)\n",
    "      return img, label\n",
    "\n",
    "\n",
    "    # Make the imgs in to batches\n",
    "    def configure_for_performance(ds):\n",
    "      ds = ds.cache()\n",
    "      ds = ds.shuffle(buffer_size=1000)\n",
    "      ds = ds.batch(1)\n",
    "      ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "      return ds\n",
    "\n",
    "    data_dir = pathlib.Path(path) # for pycharm\n",
    "    #data_dir = pathlib.Path('mnist_png/training') # for the jupyter notebook\n",
    "\n",
    "    #print(path+'\\\\*\\\\*')\n",
    "\n",
    "\n",
    "    list_ds = tf.data.Dataset.list_files(path, shuffle=True)\n",
    "\n",
    "    image_count = len(list_ds)\n",
    "    class_name = [int(tf.strings.split(path, os.path.sep)[-2])]\n",
    "    #print(f\"Class -> {class_name}\")\n",
    "    val_size = int(image_count)\n",
    "    val_ds = list_ds.take(val_size)\n",
    "    #print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "\n",
    "\n",
    "    # Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "    val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    val_ds = configure_for_performance(val_ds)\n",
    "\n",
    "\n",
    "    return val_ds, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a3296f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-gmateus@ilof.tech/Classifier_AI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#os.chdir('D:\\AI_Projects\\Classifier_AI')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5f2cb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_png/training/*/*\n",
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "48000\n",
      "12000\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:08:50.415531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:50.415996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:50.416049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:50.416321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 23:08:50.417460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:50.417521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:50.417562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:51.067086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:51.067158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:51.067199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 23:08:51.067239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9546 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "normalized_ds, val_ds, class_names = create_dataset_from_path('mnist_png/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa88fe7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (28, 28, 1)\n",
      "Label:  2\n",
      "Label:  [2 3 9 1 5 6 7 2 3 4 4 9 4 6 6 5 9 2 9 5 3 7 7 1 9 5 5 1 1 8 7 4]\n",
      "Image shape:  (28, 28, 3)\n",
      "Label:  2\n",
      "0.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:08:52.775342: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABX0lEQVR4nO2UPUsDQRCGn3g5BA0INgEDEQQFFSsbLWxVoqA2aiMBGxHEJo22+hMCKVKkifgPFEshgoKgNqKCjaA2fnQWgqLFMWE2l03inWKTlyn2ZmaffdnbXWjqX5SBLyjCFoxDEtyQxCTcwKcZl7AQmNgN1z5imdsVjHhlIXqRt0xsqZp1YBUOoE8y73AH25AN4M5bJ2M6uoc1qS6p/FTj0LRJPIWElHrgQvKH0G63ValONT6DWXiQzwIMyfgE3hp3GoWURELlJ+FZbO7Ybf5AA/Ck9mQwPBHIKWLhV2xOw4sQS+GJMZiDY2VzxmxIwRFkbQdeqxUWoei79XloMzsfpVTniRmGXcu9HDM7e+G1EagD+/bLvq465+FWlTQ0WgEdgQn7kivwIYN+cCS/LPnq6qj5LFWNNERqED3n5X+9B6MQgw0L7hzidYmeHHDBVackApsmLgdx/9419Xf6BvE/lsjUjwgXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the normalized\n",
    "for imageBatch, labelBatch in normalized_ds.take(1):\n",
    "    print(\"Image shape: \", imageBatch.numpy()[0].shape)\n",
    "    print(\"Label: \", labelBatch.numpy()[0])\n",
    "    print(\"Label: \", labelBatch.numpy())\n",
    "    #print(np.min(image), np.max(image))\n",
    "\n",
    "    # f = open(\"demofile.txt\", \"a\")\n",
    "    # for a in range(28): # goes from 0 to 27\n",
    "    #   for b in range(28):\n",
    "    #     print(f'{round(float(image[b][a][0]), 1)}',end='')\n",
    "    #     f.write(str(round(float(image[b][a][0]), 1)))\n",
    "    #   print(f'\\n')\n",
    "    #   f.write('\\n')\n",
    "    # f.close()\n",
    "\n",
    "    imgToBeDisplayed = imageBatch.numpy()[0]\n",
    "\n",
    "    extraChannel = np.zeros((28,28,1))\n",
    "    imgToBeDisplayed = np.concatenate((imgToBeDisplayed,extraChannel), axis=2)\n",
    "    imgToBeDisplayed = np.concatenate((imgToBeDisplayed,extraChannel), axis=2)\n",
    "    imgToBeDisplayed = imgToBeDisplayed * 255\n",
    "\n",
    "    print(\"Image shape: \", imgToBeDisplayed.shape)\n",
    "    print(\"Label: \", labelBatch.numpy()[0])\n",
    "    # #print(f'->{image}')\n",
    "\n",
    "    print(np.min(imageBatch[0]), np.max(imageBatch[0]))\n",
    "\n",
    "    image_uint8 = imgToBeDisplayed.astype(np.uint8)\n",
    "    img = Image.fromarray(image_uint8, 'RGB')\n",
    "    #img.show()\n",
    "    #img.save('Classifier_AI\\\\temp.png')\n",
    "    img.save('temp.png')\n",
    "\n",
    "#PIL.Image.open('Classifier_AI\\\\temp.png')\n",
    "PIL.Image.open('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50290e65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.remove('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a19c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(filters=15, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28, 28, 1),\n",
    "                         data_format=\"channels_last\"),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "  tf.keras.layers.Conv2D(filters=20, kernel_size=(10,10), padding='Same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "  tf.keras.layers.Conv2D(filters=25, kernel_size=(15,15), padding='Same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b483aa48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d301b125",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:08:58.103373: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  49/1500 [..............................] - ETA: 1s - loss: 1.4155 - accuracy: 0.5134   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:08:58.313334: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-04 23:08:58.313679: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-04 23:08:58.313699: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-04-04 23:08:58.314011: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-04 23:08:58.314046: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-04-04 23:08:58.322711: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 1ms/step - loss: 0.1669 - accuracy: 0.9461 - val_loss: 0.0590 - val_accuracy: 0.9818\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 1s 982us/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.0392 - val_accuracy: 0.9878\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 1s 950us/step - loss: 0.0328 - accuracy: 0.9895 - val_loss: 0.0288 - val_accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 1s 918us/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0190 - val_accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 1s 917us/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0178 - val_accuracy: 0.9943\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 1s 970us/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.0208 - val_accuracy: 0.9942\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 1s 976us/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.0179 - val_accuracy: 0.9951\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 1s 972us/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0383 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 1s 932us/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.0223 - val_accuracy: 0.9942\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 1s 939us/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0202 - val_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "history = model.fit(\n",
    "  normalized_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e40063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 15)        390       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 15)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 20)        30020     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 20)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 25)          112525    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 225)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               28928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,153\n",
      "Trainable params: 173,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_models')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1385e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74847abb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, to_file='model.png',\n",
    "#                          show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ee2361",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0788f017",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:840\u001b[0m, in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    831\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m--> 840\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:383\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mnew_figure_manager(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:361\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m():\n\u001b[1;32m    360\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _get_required_interactive_framework(\u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_native_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m threading\u001b[38;5;241m.\u001b[39mget_native_id() \u001b[38;5;241m!=\u001b[39m threading\u001b[38;5;241m.\u001b[39mmain_thread()\u001b[38;5;241m.\u001b[39mnative_id:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:208\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _backend_mod\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:279\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    276\u001b[0m     current_framework \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39m_get_running_interactive_framework()\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (current_framework \u001b[38;5;129;01mand\u001b[39;00m required_framework\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m current_framework \u001b[38;5;241m!=\u001b[39m required_framework):\n\u001b[0;32m--> 279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load backend \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m which requires the \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m interactive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework, as \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is currently running\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    282\u001b[0m                 newbackend, required_framework, current_framework))\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Load the new_figure_manager() and show() functions from the backend.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Classically, backends can directly export these functions.  This should\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# keep working for backcompat.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m new_figure_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(backend_mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_figure_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a808b46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(img_path):\n",
    "\n",
    "    test_val_ds, test_class_names = create_dataset_from_img(img_path)\n",
    "\n",
    "    predictions = model.predict(test_val_ds)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    answerBool = (int(class_names[np.argmax(score)]) == test_class_names[0])\n",
    "\n",
    "    print(\n",
    "        \"[{}] This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(answerBool, class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "\n",
    "    for imageBatch, labelBatch in test_val_ds.take(1):\n",
    "        imgToBeDisplayed = imageBatch.numpy()[0]\n",
    "\n",
    "        extraChannel = np.zeros((28,28,1))\n",
    "        imgToBeDisplayed = np.concatenate((imgToBeDisplayed,extraChannel), axis=2)\n",
    "        imgToBeDisplayed = np.concatenate((imgToBeDisplayed,extraChannel), axis=2)\n",
    "        imgToBeDisplayed = imgToBeDisplayed * 255\n",
    "\n",
    "        image_uint8 = imgToBeDisplayed.astype(np.uint8)\n",
    "        img = Image.fromarray(image_uint8, 'RGB')\n",
    "        #img.show()\n",
    "        img.save('temp.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d424eb0e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[True] This image most likely belongs to 6 with a 100.00 percent confidence.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABBElEQVR4nO3TTytEcRTG8U9Y3BSxUKSYRBQ2srBQyiyUN2AxL8PO1htgO29ANpYWCjULG1nPgiIL05Q/iymF8cvipkYT3XvdZjVPZ/X8Ot/znE4/OqwB7jhJ39j3x1vERKY0PZm6/gHdAof5DjwlsJIjcYpnAmPpe39dv8BQ1kAdP1QX+sJ77tALntrMfopUqbLHSPJp6wQeGP7pF7gltFSDhYRJ3/hklKjFXOSKSc7YZZ5zIgaTh71p+6Y7BB4ZB0vUuU5OxAGBOjPfziVNlulljQoNNlJBZ7kncESJEk0+2KZC4JXNVMRY021niavJMasZiLHmKFOmRpVAjf3MuK66yqAvK44/5n4VIqoAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_model('D:\\\\AI_Projects\\\\Classifier_AI\\\\mnist_png\\\\testing\\\\6\\\\156.png')\n",
    "test_model('/home/jupyter-gmateus@ilof.tech/Classifier_AI/mnist_png/testing/6/156.png')\n",
    "PIL.Image.open('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac75459",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "Found 10000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "img_tensor = convert_tensor(img)\n",
    "\n",
    "print(img_tensor.shape)\n",
    "testing_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    pathlib.Path('mnist_png/testing'),\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cc2766",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 323us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4664.8027 ,   -64.41077,  9443.199  , ...,   -79.61272,\n",
       "        -5259.7104 , -7442.3696 ],\n",
       "       [-2168.943  ,  -323.61227,  4686.1377 , ...,   544.9698 ,\n",
       "        -1086.2988 , -1132.8331 ],\n",
       "       [-2628.83   ,   559.7561 ,   354.7001 , ...,  6772.3228 ,\n",
       "        -3000.367  , -1558.6617 ],\n",
       "       ...,\n",
       "       [ -696.9902 ,  -187.53683,  -520.27124, ...,  2715.7966 ,\n",
       "         -421.96918,   264.89276],\n",
       "       [ 3337.8408 , -2763.7156 ,  -627.5956 , ...,  -133.08769,\n",
       "         -581.16895,  1596.3546 ],\n",
       "       [-2239.03   , -1153.796  , -1198.9889 , ..., -1286.4437 ,\n",
       "          327.44672, -1133.6217 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testing_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60695b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu-shared",
   "language": "python",
   "name": "tensorflow-gpu-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
